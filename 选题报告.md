<h1 style="text-align: center;">å¼€é¢˜æŠ¥å‘Š</h1>

<div style="text-align: center; font-weight: bold;">
    ç ”ç©¶ä¸»é¢˜ï¼šåŸºäºåŠ¨æ€è®¡ç®—æ¡†æ¶çš„ç¥ç»ç½‘ç»œç®—å­å¹¶è¡ŒåŠ é€Ÿåº“â€”â€”mytorch
</div>

### **é€‰é¢˜åŠåŸºæœ¬ä¿¡æ¯**

**ä¸»é¢˜ï¼š**

**å›¢é˜Ÿæˆå‘˜ä¿¡æ¯ï¼š**ï¼ˆé¡ºåºæ— å…ˆåä¹‹åˆ†ï¼‰

| å§“å   | å­¦å·     |
| ------ | -------- |
| é™ˆå…´å¹³ | 22336037 |
| åˆ˜åå£¹ | 22336149 |
| ç½—å¼˜æ° | 22336173 |

**å›¢é˜Ÿåä½œæ–¹å¼ï¼š**é€šè¿‡ Github  çš„ç§æœ‰ä»“åº“ï¼Œå…±åŒç¼–è¾‘ä»£ç ä»¥åŠæŠ¥å‘Šã€‚

### **é€‰é¢˜è¯´æ˜**

#### **1. é€‰é¢˜èƒŒæ™¯**

æ·±åº¦å­¦ä¹ çš„çƒ­æ½®å¼€å§‹äºAlexNetå¼€å§‹ï¼Œéšç€æ•°æ®å’Œç®—åŠ›çš„ä¸æ–­æé«˜ï¼Œç§‘ç ”å’Œå·¥ç¨‹é¢†åŸŸæ€¥éœ€è¦æ·±åº¦å­¦ä¹ æ¡†æ¶åŠ é€Ÿæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ã€‚ç”±äºæ·±åº¦å­¦ä¹ ä¸»è¦è®¾è®¡çŸ©é˜µçš„ä¸€ç³»åˆ—è®¡ç®—ï¼Œéå¸¸é€‚åˆè¿ç”¨å¹¶è¡Œç®—æ³•ä»¥åŠå¹¶è¡Œæ¡†æ¶åŠ é€Ÿç›¸å…³ç®—å­çš„è®¡ç®—ã€‚ä»Caffe,Tensorflow, åˆ°PyTorch, DLFç»è¿‡åå¹´çš„å‘å±•ï¼Œä»åˆ›æ–°å„èµ·ï¼Œåˆ°PyTorchå¼€å§‹å æ®ä¸»æµï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º(æ¥è‡ªPaperWithCode)åˆ°ä»Šå¤©å·²ç»è¶…è¿‡äº†50%ä»¥ä¸Šçš„æ·±åº¦å­¦ä¹ github repoç”±PyTorchå¼€å‘ã€‚æ·±å…¥å­¦ä¹ PyTorchçš„å®ç°åŸç†ï¼Œä»å¤´åˆ°å°¾è®¾è®¡ä¸€ä¸ªç±»ä¼¼çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯¹äºæé«˜ç³»ç»Ÿå¼€å‘èƒ½åŠ›ï¼Œå¹¶è¡Œç¨‹åºï¼Œç®—æ³•è®¾è®¡èƒ½åŠ›éƒ½æœ‰å¾ˆå¤§çš„å¸®åŠ©ã€‚
![table 1](/static/ComparisonOfDLF.png)
å¦å¤–ï¼Œè™½ç„¶å¤§éƒ¨åˆ†çš„æ¡†æ¶éƒ½æ˜¯å¼€æºçš„ï¼Œä½†æ˜¯å¯¹äºå›½äº§åŒ–çš„è®¾å¤‡ï¼Œæ¯”å¦‚æ‘©å°”çº¿ç¨‹ï¼Œåä¸ºï¼Œå¯’æ­¦çºªçš„è®¡ç®—å¡ï¼ŒPyTorchç›¸å…³çš„é€‚é…å¹¶ä¸å®Œå–„ã€‚åŒæ—¶éšç€å¤§æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä¸€äº›æ–°çš„ç®—å­é€‚é…æ¯”è¾ƒæ…¢ï¼Œæ¯”å¦‚FlashAttention, ä¼ä¸šæœ‰å†…éƒ¨å®šåˆ¶ç®—å­çš„éœ€æ±‚ï¼ŒåŒæ—¶éœ€è¦å¤šå¡ï¼Œå¤šæœºäº’è”çš„æœºåˆ¶ï¼Œ ç›¸å…³å‰æ²¿å‘å±•å€¼å¾—ç ”ç©¶ï¼Œè¿™ä¹Ÿæ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä¸€ä¸ªé‡è¦å‘å±•æ–¹å‘ã€‚
#### **2. ç›¸å…³å·¥ä½œ**

> å› ä¸ºæˆ‘ä»¬çš„ç ”ç©¶ä¸»è¦åŸºäºpytorchï¼Œæ‰€ä»¥ä¸»è¦ä»‹ç»pytorchç›¸å…³çš„æƒ…å†µ
>
> ç¬¬äºŒç‚¹ç®€å•ä»‹ç»ä¸€äº›å‰æ²¿è®ºæ–‡é€šè¿‡è°ƒæ•´æˆ–è€…å†™åº•å±‚ç®—å­æ¥å®ç°åŠ é€Ÿçš„ä¾‹å­ã€‚æ¯”å¦‚deepseekå›¢é˜Ÿæå‡ºçš„ä¸€äº›èŠ‚çº¦ç¡¬ä»¶å¼€é”€çš„åŠ é€Ÿæ–¹æ–¹æ³•



**2.1 pytorch**

åœ¨å½“ä»Šç¥ç»ç½‘ç»œç ”ç©¶é¢†åŸŸï¼ŒPyTorchå·²æˆä¸ºä¸å¯æˆ–ç¼ºçš„æ ¸å¿ƒå·¥å…·ã€‚ä½œä¸ºMetaï¼ˆåŸFacebookï¼‰å¼€å‘çš„å¼€æºæ¡†æ¶ï¼Œå®ƒå‡­å€ŸåŠ¨æ€å›¾ä¼˜å…ˆçš„è®¾è®¡å“²å­¦è„±é¢–è€Œå‡ºï¼Œé€šè¿‡`torch.autograd`åœ¨Pythonè¿è¡Œæ—¶åŠ¨æ€æ„å»ºè®¡ç®—å›¾ï¼Œè¿™ç§æœºåˆ¶ç›¸æ¯”TensorFlowç­‰é™æ€å›¾æ¡†æ¶æ›´èƒ½æ»¡è¶³ç§‘ç ”åœºæ™¯çš„å¿«é€Ÿè¿­ä»£éœ€æ±‚ã€‚PyTorchä¸ä»…æä¾›ç›´è§‚çš„Pythonicç¼–ç¨‹ä½“éªŒå’Œé«˜æ•ˆçš„GPUåŠ é€Ÿèƒ½åŠ›ï¼Œè¿˜é›†æˆäº†å®Œæ•´çš„æ·±åº¦å­¦ä¹ å·¥å…·é“¾ï¼ˆå¦‚TorchVisionã€TorchTextï¼‰ï¼Œå¹¶ä¸Pythonç”Ÿæ€æ— ç¼å¯¹æ¥ï¼ŒåŒæ—¶é€šè¿‡TorchScriptå’ŒONNXæ”¯æŒå®ç°ä¾¿æ·çš„æ¨¡å‹éƒ¨ç½²ã€‚éšç€PyTorch 2.0å¼•å…¥ç¼–è¯‘ä¼˜åŒ–æŠ€æœ¯å¹¶æŒç»­å¼ºåŒ–åˆ†å¸ƒå¼è®­ç»ƒä¸å¤§æ¨¡å‹æ”¯æŒï¼ˆå¦‚Llamaï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒç§‘ç ”çµæ´»æ€§çš„åŒæ—¶ä¸æ–­æå‡å·¥ä¸šçº§æ€§èƒ½ï¼Œå·²æˆä¸ºè´¯ç©¿ç®—æ³•æ¢ç´¢åˆ°ç”Ÿäº§è½åœ°çš„é¦–é€‰å¹³å°ã€‚

ä»¥ä¸‹æ˜¯ pytorch çš„æ¡†æ¶å›¾ï¼ˆä½¿ç”¨ Mermaid ä»£ç ç¼–å†™ï¼‰ï¼š

```mermaid
graph TD
    subgraph Pythonæ¥å£
        TorchAPI["torch (æ ¸å¿ƒAPI)"]
        TorchNN["torch.nn (ç¥ç»ç½‘ç»œå±‚)"]
        TorchAutograd["torch.autograd (è‡ªåŠ¨å¾®åˆ†)"]
        TorchOptim["torch.optim (ä¼˜åŒ–å™¨)"]
        TorchDistributed["torch.distributed (åˆ†å¸ƒå¼)"]
        TorchJIT["torch.jit (JITç¼–è¯‘å™¨æ¥å£)"]
    end

    subgraph åº•å±‚C++å®ç°
        TorchC["torch._C (C++æ ¸å¿ƒ)"]
        TorchCSRC["torch/csrc (C++æ‰©å±•)"]
        JITImpl["torch/csrc/jit (JITæ ¸å¿ƒ)"]
        ATen["aten (Tensorå’Œç®—å­å®ç°)"]
        C10["c10 (é€šç”¨åŸºç¡€åº“)"]
    end

    subgraph ç¼–è¯‘ä¸åŠ é€Ÿ
        Dynamo["torch.compile & Dynamoç¼–è¯‘å™¨"]
        Inductor["TorchInductor (é«˜æ€§èƒ½åç«¯)"]
    end

    subgraph å·¥å…·ä¸æ–‡æ¡£
        Docs["docs/ (æ–‡æ¡£)"]
        Tools["tools/ (å¼€å‘ä¸CIå·¥å…·)"]
    end

    TorchAPI --> TorchNN
    TorchAPI --> TorchAutograd
    TorchAPI --> TorchOptim
    TorchAPI --> TorchDistributed
    TorchAPI --> TorchJIT

    TorchAPI --> TorchC
    TorchNN --> TorchC
    TorchAutograd --> TorchC
    TorchOptim --> TorchC
    TorchDistributed --> TorchC
    TorchJIT --> JITImpl

    TorchC --> ATen
    TorchC --> C10
    TorchCSRC --> ATen
    TorchCSRC --> C10

    TorchJIT --> JITImpl

    TorchAPI --> Dynamo
    Dynamo --> Inductor

    Docs -.-> TorchAPI
    Docs -.-> TorchC
    Tools -.-> TorchAPI
    Tools -.-> TorchC

```

### 2.2 å‰æ²¿å‘å±•ä¸æŒ‘æˆ˜ï¼šä»é«˜æ€§èƒ½ç®—å­åˆ°åˆ†å¸ƒå¼è®­ç»ƒ

è¿‘å¹´æ¥ï¼Œéšç€åŸºäº Transformer æ¶æ„çš„å¤§æ¨¡å‹ï¼ˆå¦‚ LLMï¼‰è¿…çŒ›å‘å±•ï¼Œæ·±åº¦å­¦ä¹ æ¡†æ¶ä¹Ÿé¢ä¸´æ–°çš„æ€§èƒ½ç“¶é¢ˆä¸æ¶æ„æŒ‘æˆ˜ã€‚ä¸ºæå‡è®­ç»ƒä¸æ¨ç†æ•ˆç‡ï¼Œç ”ç©¶è€…å’Œå·¥ä¸šç•Œä»å¤šä¸ªå±‚é¢æ¨è¿›æ¡†æ¶ä¼˜åŒ–ï¼Œæ¶‰åŠ**æ–°å‹é«˜æ€§èƒ½ç®—å­å¼€å‘ã€ç®—å­èåˆæœºåˆ¶ã€AI ç¼–è¯‘å™¨èåˆ**ä»¥åŠ**åˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒç­–ç•¥**ç­‰ã€‚

####  é«˜æ€§èƒ½ç®—å­ä¸ PyTorch æ‰©å±•çš„éœ€æ±‚

ä»¥ FlashAttention [1]ã€Memory Efficient Attention [2] å’Œ Online-Softmax [3] ç­‰ä¸ºä»£è¡¨çš„ä¼˜åŒ–æ³¨æ„åŠ›ç®—å­ï¼Œæ­£æˆä¸ºå¤§æ¨¡å‹ä¼˜åŒ–çš„å…³é”®è·¯å¾„ã€‚æ¯”å¦‚ï¼š

- **FlashAttention** é€šè¿‡åœ¨ CUDA kernel ä¸­å®ç° fused softmax-attentionï¼Œæ˜¾è‘—å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°ï¼Œç›¸æ¯” PyTorch åŸç”Ÿ `nn.MultiheadAttention` æ‹¥æœ‰æ›´ä½çš„æ˜¾å­˜å¼€é”€å’Œæ›´é«˜çš„ååã€‚
- ä»¥ DeepSeek å›¢é˜Ÿä¸ºä¾‹ï¼Œå…¶æœ€æ–°æ¨¡å‹ **DeepSeekMoE** å’Œ **DeepSeek-V3** åˆ†åˆ«åœ¨ç¨€ç–ä¸“å®¶æ¨¡å‹å’Œæ³¨æ„åŠ›ä¼˜åŒ–æ–¹é¢å–å¾—çªç ´ [4][5]ã€‚å°¤å…¶ DeepSeekMoE ä¸­æå‡ºçš„åŠ¨æ€ä¸“å®¶é€‰æ‹©æœºåˆ¶å’Œå¼‚æ„ä¸“å®¶è°ƒåº¦ç­–ç•¥ï¼Œå……åˆ†æš´éœ²äº† PyTorch åœ¨ç¨€ç–è®¡ç®—å’Œè·¨è®¾å¤‡é€šä¿¡æ–¹é¢çš„çŸ­æ¿ã€‚

ç„¶è€Œï¼ŒPyTorch å¯¹è¿™ç±»æ–°ç®—å­çš„åŸç”Ÿæ”¯æŒå¾€å¾€æ»åï¼Œä¼ä¸šä¸ç ”ç©¶æœºæ„é€šå¸¸éœ€è¦ï¼š

- ç¼–å†™è‡ªå®šä¹‰ **C++/CUDA æ‰©å±•**ï¼ˆå¦‚ FlashAttentionï¼‰ã€‚
- ä½¿ç”¨ **Triton/TorchInductor** å®ç°æ–°çš„ fused kernelã€‚
- å°è£…ä¸º `torch.library` è‡ªå®šä¹‰ç®—å­ï¼Œä¾›é«˜å±‚æ¨¡å—è°ƒç”¨ã€‚

PyTorch å¯¹äºè¿™ç±»å‰æ²¿ç®—å­çš„æ”¯æŒç›®å‰ä¾èµ–ç¤¾åŒºè´¡çŒ®ï¼Œæ•´åˆè¿›ä¸»å¹²ä»£ç çš„å‘¨æœŸè¾ƒé•¿ï¼Œå› æ­¤å…¶é€‚åº”èƒ½åŠ›å­˜åœ¨ä¸€å®šæ»åã€‚

#### âš™ï¸ ç®—å­èåˆï¼šä» kernel fusion åˆ° graph-level ç¼–è¯‘

ç®—å­èåˆæ˜¯å½“å‰ä¼˜åŒ–æ¡†æ¶æ€§èƒ½çš„æ ¸å¿ƒæ‰‹æ®µä¹‹ä¸€ã€‚ä¼ ç»Ÿ PyTorch ä¸­ï¼Œå¤šä¸ªç®—å­æ‰§è¡Œéœ€è¦å¤šæ¬¡ CUDA kernel launchï¼Œå¸¦æ¥æ˜¾è‘—çš„ overheadã€‚è€Œç°ä»£ AI æ¨¡å‹å¾€å¾€å…·æœ‰å¤§é‡å°è§„æ¨¡æ“ä½œï¼ˆå¦‚ `LayerNorm + GELU + residual`ï¼‰ï¼Œå¦‚æœä¸èƒ½æœ‰æ•ˆèåˆï¼Œå°±ä¼šå¯¼è‡´ä¸¥é‡çš„ GPU under-utilizationã€‚

ä¸ºæ­¤ï¼Œç¤¾åŒºå‘å±•å‡ºä»¥ä¸‹æŠ€æœ¯è·¯å¾„ï¼š

- **Fused kernel**ï¼ˆå¦‚ Apex çš„ fused layernormã€xFormers ä¸­çš„ fused MHAï¼‰ã€‚
- **Triton**ï¼š[OpenAI Triton](https://github.com/openai/triton) å…è®¸ç”¨ Python å†™ GPU kernelï¼Œå®ç° block-level fusionã€‚
- **TorchInductor / TorchDynamo**ï¼šPyTorch 2.x å¼•å…¥çš„æ–°ç¼–è¯‘å­ç³»ç»Ÿï¼Œæ”¯æŒè‡ªåŠ¨è¿½è¸ªè®¡ç®—å›¾ã€ç”Ÿæˆ IRï¼Œå¹¶åˆ©ç”¨ Inductor è‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆçš„ fused kernelã€‚

æ­¤å¤–ï¼ŒPyTorch ä¹Ÿåœ¨å­¦ä¹  TVM / XLA ç­‰ç¼–è¯‘å™¨æ¡†æ¶ä¸­çš„ä¼˜åŠ¿ï¼Œå°è¯•é€šè¿‡ `torch.compile()` å’Œ `torch._dynamo` æä¾›é™æ€å›¾å’Œæ··åˆå›¾ç¼–è¯‘èƒ½åŠ›ã€‚ä½†ç›®å‰çš„æŒ‘æˆ˜åŒ…æ‹¬ï¼š

- Kernel fusion è‡ªåŠ¨åŒ–ç¨‹åº¦ä»ç„¶æœ‰é™ï¼Œä¾èµ–å¼€å‘è€…ç¼–å†™ patternã€‚
- Triton ç­‰å·¥å…·çš„é€šç”¨æ€§å°šä¸å¦‚ä¼ ç»Ÿ CUDA å·¥å…·é“¾ï¼ˆå¦‚ cuDNNï¼‰ã€‚

#### ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒï¼šæ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œä¸é€šä¿¡ä¼˜åŒ–

éšç€å‚æ•°è§„æ¨¡ä»äº¿çº§æ‰©å±•åˆ°ç™¾äº¿ã€ä¸‡äº¿çº§åˆ«ï¼Œ**åˆ†å¸ƒå¼è®­ç»ƒæœºåˆ¶**æˆä¸º PyTorch æ¡†æ¶æ€§èƒ½çš„å¦ä¸€å…³é”®å› ç´ ã€‚ç›®å‰ PyTorch çš„æ ¸å¿ƒæ”¯æŒåŒ…æ‹¬ï¼š

| å¹¶è¡Œæ–¹å¼ | PyTorchå·¥å…·/ç”Ÿæ€ | ä¼˜ç¼ºç‚¹ |
|----------|------------------|--------|
| æ•°æ®å¹¶è¡Œ (DDP) | `torch.nn.parallel.DistributedDataParallel` | ç¨³å®šæˆç†Ÿï¼Œä½†é€šä¿¡å¼€é”€å¤§ |
| æ¨¡å‹å¹¶è¡Œ (MP) | Megatron-LM, deepspeed-pipe, fairscale | æ§åˆ¶éš¾åº¦å¤§ï¼Œæ‰‹åŠ¨åˆ†å‰²æ¨¡å‹ |
| å¼ é‡å¹¶è¡Œ (TP) | Megatron-LM, Colossal-AI | æ”¯æŒå¤§å¼ é‡åˆ‡ç‰‡ï¼Œä½†å®ç°å¤æ‚ |
| ä¸“å®¶å¹¶è¡Œ (MoE) | DeepSpeed-MoE, GShard | è°ƒåº¦å¤æ‚ï¼Œéœ€è¦ç¨€ç–é€šä¿¡ä¼˜åŒ– |
| æ··åˆå¹¶è¡Œ (FSDP) | `torch.distributed.fsdp` | é«˜æ•ˆä½†è°ƒè¯•æˆæœ¬é«˜ï¼Œæ¥å£ä¸ç¨³å®š |

è™½ç„¶ PyTorch å·²æ”¯æŒ FSDPã€DDP å’Œ RPC ç­‰æœºåˆ¶ï¼Œä½†åœ¨ä»¥ä¸‹æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼š

- é€šä¿¡ç®—å­ï¼ˆå¦‚ NCCL all_reduceï¼‰æ˜“æˆä¸ºç“¶é¢ˆï¼Œç¼ºä¹é«˜çº§èåˆä¼˜åŒ–ã€‚
- MoE ç­‰ç¨€ç–å¹¶è¡Œæœºåˆ¶æ”¯æŒæœ‰é™ï¼Œéœ€ä¾èµ– DeepSpeedã€FairScale ç­‰å¤–éƒ¨å·¥å…·ã€‚
- ä¸åŒå¹¶è¡Œç­–ç•¥ä¹‹é—´çš„ç»„åˆæ”¯æŒï¼ˆå¦‚ TP + MoE + PPï¼‰ä»ç¼ºä¹é€šç”¨æ¥å£ã€‚






#### **3. ç ”ç©¶æ„ä¹‰**

> å¯¹äº†è§£pytorchçš„å®ç°ï¼Œåº”ç”¨æˆ‘ä»¬çš„å¹¶è¡ŒçŸ¥è¯†æœ‰å¾ˆå¤§çš„å¸®åŠ©ï¼Œè€Œä¸”åœ¨é”™ç»¼å¤æ‚çš„å›½é™…å½¢åŠ¿ä¸‹ï¼Œæå‡ºæ›´å¥½çš„åŠ é€Ÿæ–¹æ¡ˆä»¥åŠè®¡ç®—æ¡†æ¶å›½äº§åŒ–å…·æœ‰éå‡¡çš„æ„ä¹‰ã€‚ï¼ˆå°±æ˜¯ä»ä¸ªäººåˆ°å›½å®¶çš„è§’åº¦ç®€å•è¯´æ˜ï¼‰



### **é¢„æœŸæˆæœ**

å› ä¸ºé€‰é¢˜å®ç°çš„å¤æ‚ç¨‹åº¦åŠéš¾åº¦è¾ƒå¤§ï¼Œæš‚ä¸çŸ¥é“èƒ½åšåˆ°ä»€ä¹ˆå±‚æ¬¡çš„æˆæœã€‚æ•…æˆ‘ä»¬çš„é¢„æœŸç»“æœé‡‡ç”¨åˆ†å±‚æ–¹å¼å±•ç°ã€‚

- **åˆçº§ç›®æ ‡ï¼š**
  - [ ] å­¦ä¹ å¹¶å¤ç° pytorch çš„ç›¸å…³åŸºç¡€ä»£ç ï¼Œæ¯”å¦‚å¼ é‡æ„å»ºã€æ•°æ®å¤„ç†ç­‰
  - [ ] å®ç°ä¸€ä¸ªå…¨è¿æ¥å±‚ç½‘ç»œçš„è®­ç»ƒç›¸å…³ä»£ç 
- **ä¸­çº§ç›®æ ‡ï¼š**
  - [ ] äº†è§£å¹¶å®ç°åˆ†å¸ƒå¼å¹¶è¡Œå†…å®¹ï¼Œæ”¯æŒå¤šå¡è¿ç®—
  - [ ] è®¾è®¡ä¸€äº›ä¸ªæ€§åŒ–ç®—å­ä»¥æ”¯æŒå®é™…åº”ç”¨ï¼Œå¹¶å–å¾—åŠ é€Ÿæ•ˆæœ
  - [ ] å®Œæˆç¥ç»ç½‘ç»œï¼Œè‡ªåŠ¨å¾®åˆ†ä»¥åŠä¼˜åŒ–å™¨çš„APIçš„è®¾è®¡å’Œå®ç°

- **é«˜çº§ç›®æ ‡ï¼š**
  - [ ] å½¢æˆä¸€ä¸ªç›¸å¯¹å®Œæ•´çš„è®¡ç®—æ¡†æ¶
  - [ ] é’ˆå¯¹Transformeræ¶æ„å®ç°èåˆç®—å­ï¼Œå–å¾—è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦çš„æå‡
  - [ ] å°è¯•é€šè¿‡ç®—å­ä¼˜åŒ–ç­‰æ–¹å¼ï¼Œæ¥åŠ é€Ÿå‰æ²¿è®ºæ–‡æ¨¡å‹


### **å‚è€ƒèµ„æ–™**

- [[1912.01703\] PyTorch: An Imperative Style, High-Performance Deep Learning Library](https://arxiv.org/abs/1912.01703)
- [[2401.06066\] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)
- [[2412.19437\] DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
- [pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration](https://github.com/pytorch/pytorch)
- [pytorchæºç ä»‹ç»-è§†é¢‘åˆé›†-å“”å“©å“”å“©è§†é¢‘](https://space.bilibili.com/373596439/lists?sid=57707&spm_id_from=333.788.0.0)

- [1] FlashAttention: https://github.com/Dao-AILab/flash-attention  
- [2] XFormers Efficient Attention: https://facebookresearch.github.io/xformers/  
- [3] Online Softmax paper: https://arxiv.org/abs/2107.12102  
- [4] DeepSeekMoE: https://huggingface.co/deepseek-ai/DeepSeekMoE  
- [5] DeepSeek-V3: https://github.com/deepseek-ai/DeepSeek-V3  
- [6] PyTorch TorchDynamo/TorchInductor: https://pytorch.org/get-started/pytorch-2.0/  
- [7] Triton OpenAI: https://github.com/openai/triton  
- [8] TVM: https://tvm.apache.org/


